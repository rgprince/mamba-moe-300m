{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba-MoE 509M - Production Training on Kaggle TPU\n",
    "\n",
    "**STEP-BY-STEP NOTEBOOK**\n",
    "\n",
    "Run each cell sequentially. You can download checkpoints at any step!\n",
    "\n",
    "- **Target**: 10GB data, ~2.5B tokens\n",
    "- **Model**: 509M parameters\n",
    "- **Hardware**: TPU v5e-8\n",
    "- **Time**: ~4-5 hours total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "%cd /kaggle/working\n",
    "!rm -rf mamba-moe-300m\n",
    "!git clone https://github.com/rgprince/mamba-moe-300m.git\n",
    "%cd mamba-moe-300m\n",
    "\n",
    "# Install dependencies (if needed)\n",
    "!pip install -q jax[tpu] flax optax chex einops pyyaml pydantic datasets sentencepiece tqdm -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Download & Prepare Training Data (10GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DOWNLOADING 10GB HIGH-QUALITY DATA\")\n",
    "print(\"=\"*70)\n",
    "print(\"Streaming mode - only downloads what we need!\\n\")\n",
    "\n",
    "# Target: 10GB total = 2M samples from FineWeb-Edu\n",
    "datasets_config = [\n",
    "    {\n",
    "        \"name\": \"HuggingFaceFW/fineweb-edu\",\n",
    "        \"split\": \"train\",\n",
    "        \"samples\": 2_000_000,  # 2M samples = ~9.5-10GB\n",
    "        \"description\": \"FineWeb-Edu: Educational web content\"\n",
    "    },\n",
    "]\n",
    "\n",
    "all_texts = []\n",
    "total_chars = 0\n",
    "\n",
    "for ds_config in datasets_config:\n",
    "    print(f\"\\nLoading {ds_config['description']}...\")\n",
    "    print(f\"Streaming {ds_config['samples']:,} samples...\")\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\n",
    "            ds_config['name'],\n",
    "            split=ds_config['split'],\n",
    "            streaming=True,\n",
    "            trust_remote_code=False\n",
    "        )\n",
    "        \n",
    "        samples_collected = 0\n",
    "        dataset_chars = 0\n",
    "        \n",
    "        pbar = tqdm(total=ds_config['samples'], desc=f\"  {ds_config['name'].split('/')[1][:20]}\")\n",
    "        \n",
    "        for item in dataset:\n",
    "            text = item.get('text') or item.get('content') or ''\n",
    "            \n",
    "            if len(text) > 100:\n",
    "                all_texts.append(text)\n",
    "                dataset_chars += len(text)\n",
    "                samples_collected += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "                if samples_collected >= ds_config['samples']:\n",
    "                    break\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        total_chars += dataset_chars\n",
    "        \n",
    "        print(f\"‚úì Collected {samples_collected:,} samples\")\n",
    "        print(f\"‚úì Size: {dataset_chars/1e6:.1f}MB ({dataset_chars/1e9:.2f}GB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Failed: {e}\")\n",
    "\n",
    "# Combine all text\n",
    "combined_text = \"\\n\\n\".join(all_texts)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DATASET SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total samples: {len(all_texts):,}\")\n",
    "print(f\"Total size: {len(combined_text)/1e6:.1f}MB ({len(combined_text)/1e9:.2f}GB)\")\n",
    "print(f\"Estimated tokens: ~{len(combined_text)/4/1e6:.1f}M tokens\")\n",
    "print(f\"\\nMemorization check:\")\n",
    "print(f\"  Model params: 509M\")\n",
    "print(f\"  Data tokens: ~{len(combined_text)/4/1e6:.1f}M\")\n",
    "print(f\"  Ratio: {(len(combined_text)/4/1e6)/509:.2f} tokens/param\")\n",
    "if (len(combined_text)/4/1e6)/509 > 2:\n",
    "    print(f\"  Status: ‚úÖ GOOD (>2) - No memorization!\")\n",
    "else:\n",
    "    print(f\"  Status: ‚ö†Ô∏è LOW (<2) - May memorize\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data ready! You can now train the tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Train Tokenizer (SentencePiece BPE)\n",
    "\n",
    "**üì• DOWNLOAD CHECKPOINT**: After this step, you can download `data/tokenizer.model` and `data/tokenizer.vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import SPTokenizer\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING TOKENIZER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save a 200MB sample for tokenizer training (prevents RAM overflow!)\n",
    "TOKENIZER_SAMPLE_SIZE = 200_000_000  # 200MB\n",
    "tokenizer_sample = combined_text[:TOKENIZER_SAMPLE_SIZE]\n",
    "\n",
    "tokenizer_train_file = data_dir / \"tokenizer_sample.txt\"\n",
    "print(f\"Saving tokenizer sample ({len(tokenizer_sample)/1e6:.1f}MB)...\")\n",
    "with open(tokenizer_train_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_sample)\n",
    "\n",
    "print(f\"\\nTraining tokenizer (vocab_size=8000)...\")\n",
    "print(f\"(Using sample - prevents RAM overflow!)\\n\")\n",
    "\n",
    "tokenizer = SPTokenizer.train(\n",
    "    input_files=[str(tokenizer_train_file)],\n",
    "    vocab_size=8000,\n",
    "    model_prefix=str(data_dir / \"tokenizer\"),\n",
    "    model_type=\"bpe\",\n",
    "    input_sentence_size=2_000_000\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Tokenizer trained!\")\n",
    "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"   Files saved: data/tokenizer.model, data/tokenizer.vocab\")\n",
    "print(f\"\\nüì• DOWNLOAD: You can download tokenizer files from data/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Tokenize Data & Create Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TOKENIZING DATA & CREATING BATCHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Config\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LEN = 128\n",
    "TOTAL_STEPS = 50000\n",
    "\n",
    "print(f\"\\nTokenizing FULL {len(combined_text):,} characters...\")\n",
    "print(f\"(Using all data for training!)\\n\")\n",
    "\n",
    "token_ids = tokenizer.encode(combined_text, add_bos=False, add_eos=False)\n",
    "print(f\"‚úì Tokenized: {len(token_ids):,} tokens\")\n",
    "\n",
    "# Create batches\n",
    "print(f\"\\nCreating {TOTAL_STEPS:,} batches...\")\n",
    "batches = []\n",
    "\n",
    "for i in range(0, len(token_ids) - SEQ_LEN - 1, SEQ_LEN):\n",
    "    if len(batches) >= TOTAL_STEPS:\n",
    "        break\n",
    "    \n",
    "    batch_input_ids = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    for b in range(BATCH_SIZE):\n",
    "        offset = i + b * SEQ_LEN\n",
    "        if offset + SEQ_LEN + 1 > len(token_ids):\n",
    "            break\n",
    "            \n",
    "        input_ids = token_ids[offset:offset + SEQ_LEN]\n",
    "        labels = token_ids[offset + 1:offset + SEQ_LEN + 1]\n",
    "        \n",
    "        batch_input_ids.append(input_ids)\n",
    "        batch_labels.append(labels)\n",
    "    \n",
    "    if len(batch_input_ids) == BATCH_SIZE:\n",
    "        batches.append({\n",
    "            'input_ids': jnp.array(batch_input_ids, dtype=jnp.int32),\n",
    "            'labels': jnp.array(batch_labels, dtype=jnp.int32)\n",
    "        })\n",
    "\n",
    "print(f\"‚úì Created {len(batches):,} batches\")\n",
    "print(f\"  Batch shape: {batches[0]['input_ids'].shape}\")\n",
    "print(f\"\\n‚úÖ Data ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Load Model (509M parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random\n",
    "from src.model import create_model_from_config, ModelConfig\n",
    "from src.training import (\n",
    "    create_train_step,\n",
    "    create_train_state,\n",
    "    create_learning_rate_schedule,\n",
    "    create_optimizer,\n",
    "    CheckpointManager,\n",
    "    ConsoleLogger\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "config_path = 'configs/model_config.yaml'\n",
    "model = create_model_from_config(config_path)\n",
    "model_config = ModelConfig.from_yaml(config_path)\n",
    "\n",
    "print(f\"\\nModel: {model_config.name}\")\n",
    "print(f\"Layers: {model_config.num_layers}\")\n",
    "print(f\"Hidden dim: {model_config.hidden_dim}\")\n",
    "\n",
    "# Initialize parameters\n",
    "print(f\"\\nInitializing parameters...\")\n",
    "rng = random.PRNGKey(42)\n",
    "rng, init_rng, dropout_rng = random.split(rng, 3)\n",
    "\n",
    "dummy_input = jnp.ones((1, SEQ_LEN), dtype=jnp.int32)\n",
    "variables = model.init(init_rng, dummy_input, deterministic=True)\n",
    "params = variables['params']\n",
    "\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print(f\"‚úì Parameters: {param_count / 1e6:.1f}M\")\n",
    "\n",
    "# Setup training\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "lr_schedule = create_learning_rate_schedule(\n",
    "    warmup_steps=100,\n",
    "    max_learning_rate=LEARNING_RATE,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    schedule_type='cosine'\n",
    ")\n",
    "\n",
    "optimizer = create_optimizer(\n",
    "    learning_rate_fn=lr_schedule,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=1.0\n",
    ")\n",
    "\n",
    "state = create_train_state(model, params, optimizer, lr_schedule, dropout_rng)\n",
    "train_step = create_train_step(model, lr_schedule)\n",
    "train_step = jax.jit(train_step)\n",
    "\n",
    "print(f\"\\n‚úÖ Model ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Train! (50,000 steps = ~4 hours)\n",
    "\n",
    "**üì• DOWNLOAD CHECKPOINTS**: Every 500 steps, checkpoints are saved to `checkpoints/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "SAVE_EVERY = 500\n",
    "LOG_EVERY = 50\n",
    "\n",
    "# Setup checkpoints\n",
    "ckpt_dir = Path(\"checkpoints\")\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "ckpt_manager = CheckpointManager(\n",
    "    checkpoint_dir=str(ckpt_dir),\n",
    "    max_to_keep=3,\n",
    "    save_interval_steps=SAVE_EVERY\n",
    ")\n",
    "\n",
    "print(f\"\\nConfig:\")\n",
    "print(f\"  Total steps: {TOTAL_STEPS:,}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Sequence length: {SEQ_LEN}\")\n",
    "print(f\"  Save every: {SAVE_EVERY} steps\")\n",
    "print(f\"  Log every: {LOG_EVERY} steps\")\n",
    "print(f\"\\n(Compiling on first step - takes ~1 min)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "logger = ConsoleLogger(log_interval=LOG_EVERY)\n",
    "start_time = time.time()\n",
    "compile_time = None\n",
    "\n",
    "for step in range(min(TOTAL_STEPS, len(batches))):\n",
    "    batch = batches[step]\n",
    "    \n",
    "    step_start = time.time()\n",
    "    state, metrics = train_step(state, batch)\n",
    "    step_time = time.time() - step_start\n",
    "    \n",
    "    if step == 0:\n",
    "        compile_time = step_time\n",
    "        print(f\"‚úì Compilation done ({compile_time:.1f}s)\\n\")\n",
    "    \n",
    "    metrics['step_time'] = step_time\n",
    "    metrics['tokens_per_sec'] = (BATCH_SIZE * SEQ_LEN) / step_time if step > 0 else 0\n",
    "    \n",
    "    # Log\n",
    "    if step % LOG_EVERY == 0 or step == 0:\n",
    "        loss = float(metrics['loss'])\n",
    "        ppl = float(metrics['perplexity'])\n",
    "        lr = float(metrics['learning_rate'])\n",
    "        tps = int(metrics['tokens_per_sec'])\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Step {step:5d} | loss={loss:.4f} ppl={ppl:8.2f} lr={lr:.6f} | {tps:,} tok/s | {elapsed/60:.1f}min\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (step + 1) % SAVE_EVERY == 0:\n",
    "        ckpt_manager.save(state, step, metadata={'loss': float(metrics['loss'])})\n",
    "        print(f\"üì• Checkpoint saved at step {step+1} - You can download from checkpoints/\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  ‚úì Trained {TOTAL_STEPS:,} steps\")\n",
    "print(f\"  ‚úì Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "print(f\"  ‚úì Final loss: {float(metrics['loss']):.4f}\")\n",
    "print(f\"  ‚úì Final perplexity: {float(metrics['perplexity']):.2f}\")\n",
    "print(f\"\\nüì• DOWNLOAD:\")\n",
    "print(f\"  - Tokenizer: data/tokenizer.model\")\n",
    "print(f\"  - Checkpoints: checkpoints/\")\n",
    "print(f\"  - Logs: (in notebook output)\")\n",
    "print(f\"\\nüéâ Training complete on {len(combined_text)/1e9:.2f}GB of clean data!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
