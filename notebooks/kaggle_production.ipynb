{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba-MoE 509M - Production Training (Pre-Downloaded Data)\n",
    "\n",
    "**Optimized for Kaggle TPU v5e-8**\n",
    "\n",
    "- Uses existing FineWeb-Edu dataset (no re-download!)\n",
    "- Checkpoints every 10K steps (saves storage)\n",
    "- ~50K total steps = 10-12 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo (or use existing)\n",
    "%cd /kaggle/working\n",
    "!git clone https://github.com/rgprince/mamba-moe-300m.git 2>/dev/null || echo \"Repo exists\"\n",
    "%cd mamba-moe-300m\n",
    "\n",
    "# Pull latest fixes\n",
    "!git pull origin main\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers\n",
    "\n",
    "print(\"\\nâœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Pre-Downloaded Dataset\n",
    "\n",
    "**Using your existing FineWeb-Edu .npy files!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING PRE-DOWNLOADED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Path to your dataset\n",
    "DATASET_PATH = Path(\"/kaggle/input/fineweb-edu-10bt-for-gpt2/train\")\n",
    "\n",
    "# Find all .npy files\n",
    "npy_files = sorted(DATASET_PATH.glob(\"fineweb_edu_*.npy\"))\n",
    "print(f\"\\nFound {len(npy_files)} dataset files\")\n",
    "\n",
    "# Load all data into memory (or use lazy loading if too big)\n",
    "all_token_ids = []\n",
    "\n",
    "for npy_file in tqdm(npy_files, desc=\"Loading files\"):\n",
    "    data = np.load(npy_file)\n",
    "    all_token_ids.extend(data.tolist())\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(all_token_ids):,} tokens\")\n",
    "print(f\"  Dataset size: {len(all_token_ids) * 2 / 1e9:.2f}GB (int16)\")\n",
    "print(f\"  Estimated training tokens: {len(all_token_ids) / 1e6:.1f}M\")\n",
    "\n",
    "# Convert to array for efficient slicing\n",
    "token_ids = np.array(all_token_ids, dtype=np.int32)\n",
    "del all_token_ids  # Free memory\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"  Min token ID: {token_ids.min()}\")\n",
    "print(f\"  Max token ID: {token_ids.max()}\")\n",
    "print(f\"  Mean: {token_ids.mean():.1f}\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING GPT-2 TOKENIZER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tokenizer_hf = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Wrapper for compatibility\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, hf_tokenizer):\n",
    "        self.tokenizer = hf_tokenizer\n",
    "        self.vocab_size = len(hf_tokenizer)\n",
    "        self.bos_id = hf_tokenizer.bos_token_id or 50256\n",
    "        self.eos_id = hf_tokenizer.eos_token_id or 50256\n",
    "        self.pad_id = hf_tokenizer.pad_token_id or 50256\n",
    "\n",
    "tokenizer = TokenizerWrapper(tokenizer_hf)\n",
    "\n",
    "print(f\"âœ… Tokenizer ready!\")\n",
    "print(f\"   Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"   Max token in data: {token_ids.max()}\")\n",
    "\n",
    "if token_ids.max() >= tokenizer.vocab_size:\n",
    "    print(f\"   âŒ ERROR: Data has tokens beyond vocab!\")\n",
    "else:\n",
    "    print(f\"   âœ… All tokens within vocab range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING TRAINING BATCHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LEN = 128\n",
    "TOTAL_STEPS = 50000\n",
    "SAVE_EVERY = 10000  # Save every 10K steps (not 500!)\n",
    "LOG_EVERY = 50\n",
    "\n",
    "print(f\"\\nConfig:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Sequence length: {SEQ_LEN}\")\n",
    "print(f\"  Total steps: {TOTAL_STEPS:,}\")\n",
    "print(f\"  Save every: {SAVE_EVERY:,} steps\")\n",
    "print(f\"  Checkpoints to save: {TOTAL_STEPS // SAVE_EVERY}\")\n",
    "\n",
    "# Create batches\n",
    "batches = []\n",
    "for i in tqdm(range(TOTAL_STEPS), desc=\"Creating batches\"):\n",
    "    offset = i * SEQ_LEN\n",
    "    \n",
    "    # Wrap around if we run out of data\n",
    "    if offset + SEQ_LEN + 1 > len(token_ids):\n",
    "        offset = offset % (len(token_ids) - SEQ_LEN - 1)\n",
    "    \n",
    "    input_ids = token_ids[offset:offset + SEQ_LEN]\n",
    "    labels = token_ids[offset + 1:offset + SEQ_LEN + 1]\n",
    "    \n",
    "    batches.append({\n",
    "        'input_ids': jnp.array([input_ids], dtype=jnp.int32),\n",
    "        'labels': jnp.array([labels], dtype=jnp.int32)\n",
    "    })\n",
    "\n",
    "print(f\"\\nâœ… Created {len(batches):,} batches\")\n",
    "print(f\"   Batch shape: {batches[0]['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Mamba-MoE Model (509M params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING MAMBA-MOE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from src.model import create_model_from_config, ModelConfig\n",
    "from src.training import (\n",
    "    create_train_step,\n",
    "    create_train_state,\n",
    "    create_learning_rate_schedule,\n",
    "    create_optimizer,\n",
    "    CheckpointManager,\n",
    "    ConsoleLogger\n",
    ")\n",
    "\n",
    "# Verify config\n",
    "import yaml\n",
    "with open('configs/model_config.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "    \n",
    "print(f\"\\nðŸ” Config check:\")\n",
    "print(f\"  Config vocab: {cfg['model']['vocab_size']}\")\n",
    "print(f\"  Tokenizer vocab: {tokenizer.vocab_size}\")\n",
    "\n",
    "if cfg['model']['vocab_size'] != tokenizer.vocab_size:\n",
    "    raise ValueError(f\"MISMATCH! Update config to {tokenizer.vocab_size}\")\n",
    "\n",
    "print(f\"  âœ… Config matches!\")\n",
    "\n",
    "# Load model\n",
    "config_path = 'configs/model_config.yaml'\n",
    "model = create_model_from_config(config_path)\n",
    "model_config = ModelConfig.from_yaml(config_path)\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Architecture:\")\n",
    "print(f\"  Name: {model_config.name}\")\n",
    "print(f\"  Layers: {model_config.num_layers}\")\n",
    "print(f\"  Hidden dim: {model_config.hidden_dim}\")\n",
    "print(f\"  Mamba state dim: {model_config.mamba.state_dim}\")\n",
    "print(f\"  MoE experts: {model_config.moe.num_experts}\")\n",
    "print(f\"  MoE layers: {len(model_config.moe.layer_indices)}\")\n",
    "\n",
    "# Initialize parameters\n",
    "print(f\"\\nðŸ”„ Initializing fresh parameters...\")\n",
    "rng = random.PRNGKey(42)\n",
    "rng, init_rng, dropout_rng = random.split(rng, 3)\n",
    "\n",
    "dummy_input = jnp.ones((1, SEQ_LEN), dtype=jnp.int32)\n",
    "variables = model.init(init_rng, dummy_input, deterministic=True)\n",
    "params = variables['params']\n",
    "\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print(f\"  âœ“ Parameters: {param_count / 1e6:.1f}M\")\n",
    "\n",
    "# Setup training\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "lr_schedule = create_learning_rate_schedule(\n",
    "    warmup_steps=100,\n",
    "    max_learning_rate=LEARNING_RATE,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    schedule_type='cosine'\n",
    ")\n",
    "\n",
    "optimizer = create_optimizer(\n",
    "    learning_rate_fn=lr_schedule,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=1.0\n",
    ")\n",
    "\n",
    "state = create_train_step(model, params, optimizer, lr_schedule, dropout_rng)\n",
    "train_step = create_train_step(model, lr_schedule)\n",
    "train_step = jax.jit(train_step)\n",
    "\n",
    "print(f\"\\nâœ… Model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train! (50K steps, save every 10K)\n",
    "\n",
    "**ðŸ“¥ Checkpoints saved every 10,000 steps** (only 5 checkpoints total!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING MAMBA-MOE TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Setup checkpoints\n",
    "ckpt_dir = Path(\"checkpoints\")\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "ckpt_manager = CheckpointManager(\n",
    "    checkpoint_dir=str(ckpt_dir),\n",
    "    max_to_keep=2,  # Only keep 2 latest (saves space!)\n",
    "    save_interval_steps=SAVE_EVERY\n",
    ")\n",
    "\n",
    "print(f\"\\nâš™ï¸ Training Config:\")\n",
    "print(f\"  Total steps: {TOTAL_STEPS:,}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Sequence length: {SEQ_LEN}\")\n",
    "print(f\"  Save every: {SAVE_EVERY:,} steps (only {TOTAL_STEPS // SAVE_EVERY} checkpoints!)\")\n",
    "print(f\"  Log every: {LOG_EVERY} steps\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"\\nðŸ’¾ Checkpoint strategy: Keep 2 latest (saves storage)\")\n",
    "print(f\"\\n(First step compiles - takes ~1 min)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "logger = ConsoleLogger(log_interval=LOG_EVERY)\n",
    "start_time = time.time()\n",
    "best_loss = float('inf')\n",
    "\n",
    "for step in range(min(TOTAL_STEPS, len(batches))):\n",
    "    batch = batches[step]\n",
    "    \n",
    "    step_start = time.time()\n",
    "    state, metrics = train_step(state, batch)\n",
    "    step_time = time.time() - step_start\n",
    "    \n",
    "    if step == 0:\n",
    "        compile_time = step_time\n",
    "        print(f\"âœ“ Compilation done ({compile_time:.1f}s)\\n\")\n",
    "    \n",
    "    metrics['step_time'] = step_time\n",
    "    metrics['tokens_per_sec'] = (BATCH_SIZE * SEQ_LEN) / step_time if step > 0 else 0\n",
    "    \n",
    "    # Track best loss\n",
    "    if float(metrics['loss']) < best_loss:\n",
    "        best_loss = float(metrics['loss'])\n",
    "    \n",
    "    # Log\n",
    "    if step % LOG_EVERY == 0 or step == 0:\n",
    "        loss = float(metrics['loss'])\n",
    "        ppl = float(metrics['perplexity'])\n",
    "        lr = float(metrics['learning_rate'])\n",
    "        tps = int(metrics['tokens_per_sec'])\n",
    "        elapsed = time.time() - start_time\n",
    "        eta = (elapsed / (step + 1)) * (TOTAL_STEPS - step - 1) / 3600\n",
    "        \n",
    "        print(f\"Step {step:5d} | loss={loss:.4f} ppl={ppl:7.1f} lr={lr:.6f} | \"\n",
    "              f\"{tps:,} tok/s | {elapsed/3600:.2f}h | ETA: {eta:.1f}h\")\n",
    "    \n",
    "    # Save checkpoint every 10K steps\n",
    "    if (step + 1) % SAVE_EVERY == 0:\n",
    "        ckpt_manager.save(state, step, metadata={'loss': float(metrics['loss'])})\n",
    "        print(f\"\\nðŸ“¥ CHECKPOINT {(step+1)//SAVE_EVERY} saved at step {step+1:,}\")\n",
    "        print(f\"   Current loss: {float(metrics['loss']):.4f} | Best: {best_loss:.4f}\")\n",
    "        print(f\"   Storage: ~{(step+1)//SAVE_EVERY * 2}GB used (keeping 2 latest)\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nðŸ“Š Results:\")\n",
    "print(f\"  âœ“ Trained {TOTAL_STEPS:,} steps on Mamba-MoE 509M\")\n",
    "print(f\"  âœ“ Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"  âœ“ Final loss: {float(metrics['loss']):.4f}\")\n",
    "print(f\"  âœ“ Best loss: {best_loss:.4f}\")\n",
    "print(f\"  âœ“ Final perplexity: {float(metrics['perplexity']):.1f}\")\n",
    "print(f\"  âœ“ Avg tokens/sec: {TOTAL_STEPS * BATCH_SIZE * SEQ_LEN / total_time:.0f}\")\n",
    "print(f\"\\nðŸ’¾ Checkpoints:\")\n",
    "print(f\"  Saved {TOTAL_STEPS // SAVE_EVERY} checkpoints total\")\n",
    "print(f\"  Location: checkpoints/\")\n",
    "print(f\"  Keeping: 2 latest (automatic cleanup)\")\n",
    "print(f\"\\nðŸŽ‰ Mamba-MoE training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
